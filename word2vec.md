# Introduce word2vec

## 首先考虑做我们如何让计算机知道词义

*思考：我们是如何知道词义的？*

### 传统观点

> 经典的语言学认为每个单词都对应着真实世界的一个或实体或抽象的事物，每个单词就是一种指代，一个符号

如果以这种方法，那我们如何让计算机知道词义？   
我们要对每个单词进行分类，追溯它所从属的类别，并将某些更具体的单词划归在它的从属之下。在这里，举一个例子（wordnet）：    

```s
pip install nltk
//nltk 是做自然语言处理的一个常用包
```

```python
import nltk
from nltk.corpus import wordnet as wn
nltk.download("wordnet")
cat = wn.synsets("cat")
print(cat)
```
输出结果：    

```python
[Synset('cat.n.01'),
 Synset('guy.n.01'),
 Synset('cat.n.03'),
 Synset('kat.n.01'),
 Synset('cat-o'-nine-tails.n.01'),
 Synset('caterpillar.n.02'),
 Synset('big_cat.n.01'),
 Synset('computerized_tomography.n.01'),
 Synset('cat.v.01'),
 Synset('vomit.v.01')]
 ```
由此可以见得，如果我们想让计算机获知一个词的意思，我们可以人为的对每个单词进行划分。    
如果我们考虑这种方法的计算机实现，如何做：   
1. 首先我们需要将词转换成向量   
2. 我们事先有一个巨大的单词表（atomic symbol）
   >词库模型是文字模型化的最常用方法。对于一个文档（document），忽略其词序和语法，句法，将其仅仅看做是一个词集合，或者说是词的一个组合，文档中每个词的出现都是独立的，不依赖于其他词是否出现，或者说当这篇文章的作者在任意一个位置选择一个词汇都不受前面句子的影响而独立选择的。词库模型可以看成是独热编码的一种扩展，它为每个单词设值一个特征值。词库模型依据是用类似单词的文章意思也差不多。词库模型可以通过有限的编码信息实现有效的文档分类和检索。
3. 采用 _one-hot_ （独热词）这项技术，将一个词表示成一串0，1向量，例如：
```python
[0, 0, 0, 1, 0, 0, 0]
```
但是有一些问题：    
- 新词在不断的产生
- 组合词种类多样
- 人工难免会有疏漏
- 人工成本比较高
- 维度过大

### 一些另类的观点

> Distributional Similarity   
> "You shall know a word by the company it keeps."
>                                     —— J.R.Firth

希望可以构造一组向量来表示表示词义，而通过这些向量可以对其上下文的单词进行预测，这些词向量可以在空间中聚集成簇，具有良好的几何特征，可以在更高维度下表示单词的词义。

---
## 如何实现我们的idea？

### 介绍两个概念

1. Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型，它被大量地用在自然语言处理（NLP）中。Word2Vec通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。
2. Embedding（嵌入）相当于一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。

### 实现方法

>Word2Vec模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。Word2Vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。基于训练数据建模的过程，我们给它一个名字叫“Fake Task”，意味着建模并不是我们最终的目的。     
上面提到的这种方法实际上会在无监督特征学习（unsupervised feature learning）中见到，最常见的就是自编码器（auto-encoder）：通过在隐层将输入进行编码压缩，继而在输出层将数据解码恢复初始状态，训练完成后，我们会将输出层“砍掉”，仅保留隐层。


目前word2vec模型主要有两种训练方法：
- Skip-Gram（给定input word来预测上下文）
- CROW（给定上下文，来预测input word）

让我们具体来介绍一下Skip-Gram这种方法。    


